# Fine-Tuning Large Language Models for Question Answering

## Description
This project implements a BERT-based Question Answering system using the SQuAD 2.0 dataset. It fine-tunes pre-trained language models to accurately answer questions based on provided contexts.

## Technologies/Concepts Used
- BERT
- SQuAD 2.0
- Fine-Tuning
- Natural Language Processing

## Performance
- F1 Score: 76.70
- Exact Match (EM) Score: 73.85

## Usage
1. Install required packages (`transformers`, `torch`, `tqdm`).
2. Mount Google Drive and download SQuAD 2.0 dataset.
3. Preprocess the data, train the model, and evaluate performance.
4. Use the trained model to answer questions on new contexts.
